<!doctype html>
<html>

<head>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<script defer src="/project/portfolio.js">
	</script>
	<title>Artist-Machine Collaboration</title>

	<link rel="stylesheet" href="/project/portfolio.css" />

</head>

<body>

 <!--hamburger nav-->
<nav aria-labelledby="hamburger">
  <button id="hamburger" class="hamburger" aria-label="Show Navigation Menu" aria-expanded="false" tabindex="0">☰</button>

  <div id="navMenu" class="navMenu hidden vh">
    <button id="closeNavMenu" class="closeBtn" aria-label="Hide Navigation Menu">×</button>
    <ul>
      <li><a href="/index.html">Portfolio</a></li>
      <li><a href="/about/about.html">About</a></li>
      <li><a href="/art/art.html">Art</a></li>
    </ul>
  </div>
</nav>
<!--hamburger nav ends-->

  <main>

<div class="centerthetext">
<h2 class="special-font-h2">Artist-Machine Collaboration</h2>
</div>
    
<div class="centerthetext">
<h3 class="titlecenter">Co-Designing Digital Human Extensions
  <br>
  to Enhance Creativity and Expressivity in Live Media Performance</h3>
</div>
    
<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
<em><strong>*This page includes a summary and certain sections from my research thesis for the MSc in Human-Computer Interaction Design programme at City, University of London, Department of Computer Science. The thesis was awarded with Distinction. Please contact me if you would like to see the full report.</strong></em>
  </div>
 </div>
</div>
</div>
<br>

<h2 class="special-font-h2"> <strong>ABOUT</strong></h2>


<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
Technological advances in the past decades have led to the development of advanced audio and visual production and performance tools, however live performance artists are still looking for new tools to enhance their creativity and expressivity.
<br>
<br>
This study reports the design process of four digital live media performance tools with the participation of a group of live media artists, using web-based virtual reality as a novel research method. It builds on prior work seeking to expand human capabilities towards enhanced creativity and expressivity, by exploring opportunities for digital human extensions which may eliminate or mitigate live performance challenges. It presents findings gathered through the analysis of a survey, semi-structured interviews, and evaluation with the participants.
<br>
<br>
Results show that intelligent machines and tools with flexible interaction models can have opportunities in the live media performance domain for the artists in search of novel tools. Artists envision intelligent tools that can become real-time support systems to resolve technical issues during the performance; creative collaborators that can inspire and suggest audio and visual patterns based on the artist’s output; and tools that can sense and augment the models of interaction.
</p>
  </div>
 </div>
</div>
</div>
<br>

<h3 class="special-fontv2-h3"> <strong>RESEARCH OBJECTIVES</strong></h3>
    
<div class="objectives">
<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
The main objective of this project is to co-design future performance tools that can become extensions of the artists to enhance their creativity and expressivity in live performance through novel features and interaction models.
<br>
<br>
The main objective encompasses the following sub-objectives:
<br>
<br>
<strong><span>&#8226;</span> RO1:</strong> Exploring artists’ experiences and existing challenges with the current technologies in live media performance
<br>
<br>
<strong><span>&#8226;</span> RO2:</strong> Exploring with artists future opportunities for digital human extensions to enhance creativity and expressivity in live media performance
<br>
<br>
<strong><span>&#8226;</span> RO3:</strong> Co-designing multiple live media performance tool concept prototypes
<br>
<br>
<strong><span>&#8226;</span> RO4:</strong> Evaluating the prototypes through web-based virtual reality simulation
<br>
<br>
</p>
  </div>
 </div>
</div>
</div>
</div>

<h3 class="special-fontv2-h3"> <strong>RESEARCH QUESTIONS</strong></h3>
<div class="research-questions">
<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
The study aims to answer the following research questions to fulfil the research objectives above:
<br><br>
<b>RQ1:</b> How do live media performance artists envisage collaboration with intelligent and expressive digital human extensions?<br><br>
<b>RQ2:</b> What are the opportunities for digital human extensions to enhance creativity and expressivity in live media performance?<br><br>
<b>RQ3:</b> How can web-based virtual reality play a role in evaluating designs?
 </div>
</div>
</div>
</div>
</div>

<h3 class="special-fontv2-h3"> <strong>BENEFICIARIES</strong></h3>
    
<div class="beneficiaries">
<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
The focus of this project is the live media performance domain; therefore, the main beneficiaries are audio and visual performance artists.
<br><br>
Other beneficiaries include:
<br><br>
<strong><span>&#8226;</span></strong> Researchers who seek to explore artist-machine collaboration and interaction, co- design methods in related research and using web-based virtual reality as a research method
<br><br>
<strong><span>&#8226;</span></strong> Audio and visual performance software/hardware companies, and their designers to gain insight on their target users’ needs and desires in live performance
<br><br>
<strong><span>&#8226;</span></strong> Event venues interested in hosting novel performances
<br><br>
<strong><span>&#8226;</span></strong> Audiences
<br>
<br>
</p>
  </div>
 </div>
</div>
</div>
</div>

<h3 class="special-fontv2-h3"> <strong>SCOPE</strong></h3>
    
<div class="scope">
<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
This project is an exploratory study which aims to produce concept designs created in collaboration with a group of practising live media (audio and visual) performance artists. The term ‘audio and visual performance’ is very broad and may refer to various forms of artistic practice such as acoustic/unamplified music performances, electronic music and video-based performances, theatre, dance, and other existing and constantly evolving forms of art performance. This research focuses primarily on live electronic audio and visual art performances, where audio and/or visuals are manipulated and performed in real-time and the use of technological tools is an essential aspect of the performance; therefore, art performances that do not utilise relevant tools are beyond the scope of this study.
<br>
<br>
The focus is on exploring future possibilities with the artists themselves and design tool concepts for the future of live media performance. Technical aspects and development of these designs are beyond the scope of this project.
<br>
<br>
</p>
  </div>
 </div>
</div>
</div>
</div>

<h3 class="special-fontv2-h3"> <strong>WORK PLAN</strong></h3>

<div class="methods">
<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
This research project was carried out over the course of 4 months, from the 1st October 2021 till the 31st January 2022.
<br>
<br>
The project started with literature review, on topics relevant to the research such as live audio and visual performance, artificial intelligence, human-machine interaction, and virtual reality as a research method. Literature review was followed by user research which involved a survey and semi-structured interviews with 7 artists.
<br>
<br>
The results shaped the prototyping stage involving multiple methods such as visualisation with 3D computer rendering, high-fidelity prototyping and virtual reality simulation. The prototypes were evaluated in a group session with a novel approach using web-based virtual reality as an evaluation method and the report was completed in the final weeks of January based on the findings.
<br>
<br>
</p>
  </div>
 </div>
</div>
</div>
</div>

<img src="/project/img/workplan.png" alt="workplan" style="width:80%" class="center" onContextMenu="return false;" draggable="false">

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
This workplan was later updated. The updates included replacing the workshops with the interviews by adapting the 4-Step Framework of the workshops and embedding a 'speculative thinking exercise' to the interviews.
 </div>
</div>
</div>
</div>
  
<h3 class="special-fontv2-h3"> <strong>THE PROCESS</strong></h3>
    
<img src="/project/img/msc-design-thinking.png" alt="workplan" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<hr class="solid">

<h2 class="special-font-h2"> <strong>EMPATHISE: USER RESEARCH</strong></h2>

<h3 class="special-fontblue-h3"> <strong>Co-Design Framework & Methods</strong></h3>

<img src="/project/img/msc-codesign.png" alt="co design framework" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<h3 class="special-fontblue-h3"> <strong>Survey</strong></h3>

<img src="/project/img/msc-survey1.png" alt="survey responses" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<h3 class="special-fontblue-h3"> <strong>Semi-Structured Interviews</strong></h3>

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
Seven interviews were conducted remotely between October 22 and November 8, 2021, due to several participants’ being located outside the UK. The nature of the semi-structured interviews was exploratory and discovery-focused.
  </div>
 </div>
</div>
</div>
</div>
    
<img src="/project/img/msc-interview1.png" alt="interview questions" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<img src="/project/img/msc-interview2.png" alt="interview questions" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<hr class="solid">

<h2 class="special-font-h2"> <strong>DEFINE</strong></h2>

<h3 class="special-fontblue-h3"> <strong>Interview Data Analysis</strong></h3>
    
<img src="/project/img/msc-analysis1.png" alt="interview data analysis" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<img src="/project/img/msc-brainstorm2.png" alt="brainstorming" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<h3 class="special-fontblue-h3""> <strong>Themes</strong></h3>

<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
The thematic analysis of the interviews was supported by the data gathered from the survey and led to three main themes with relevant sub-themes:
</p>
  </div>
 </div>
</div>

<!--theme table-->  
<table class="themetable" style="width:100%">
  <tr>
    <th>THEMES</th>
    <th>SUB-THEMES</th>
  </tr>
  <tr>
    <td class="tdbold">Live Media Performance</td>
    <td>● The Artists <br>● Experiences in Live Performance</td>
  </tr>
  <tr>
    <td class="tdbold">Artificial Intelligence in Performance</td>
    <td>● Artists' view on AI <br>● Expectations from AI Tools </td>
  </tr>
  <tr>
    <td class="tdbold">Human-Machine Interaction</td>
    <td>● Gestural Interaction and Control <br>● Remote Interaction</td>
  </tr>
</table>
<!--theme table ends-->  

<br>

<h3 class="special-fontblue-h3">Interview Data Analysis Summary</h3>

    
<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
The results show that the artists have several shared experiences and challenges, and digital human extensions as intelligent tools and tools enabling alternative interaction models have potential in addressing those and enhancing live media artists’ performances. Artists envision intelligent tools as technical and creative support systems which guide, assist, and resolve common issues to enable the artist to focus on the creative aspects of the performance; as well as inspire and suggest patterns that they can merge into their output.
<br><br>On the other hand, interaction models enabling body interaction can extend the physical capabilities to allow the artist to add more dynamism and expressivity to the performance, while remote interaction can enable integration of tools regardless of their geographical location.
  </div>
  </div>
 </div>
</div>

    
<h3 class="special-fontblue-h3"> <strong>Persona</strong></h3>

<img src="/project/img/msc-persona.png" alt="primary persona" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<hr class="solid">

<h2 class="special-font-h2"> <strong>IDEATE</strong></h2>

<h3 class="special-fontblue-h3"> <strong>5 Speculative Tools</strong></h3>

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
The 'speculative thinking exercise' encouraged the participants to imagine future tools by asking 3 speculative questions. The responses to the questions directly determined the concepts. There were only two concepts that were not considered for concept development; one did not fit in the context of ‘digital tool, intelligent tool or new interaction model, which are the areas this study focuses on. The second, although it referred to an interaction model, it was a very broad idea, the motivation wasn’t clear and unlike the other ideas, it was not shared by anyone else.
<br><br>
The responses to these questions formed the basis of the following five concepts:
<br><br>
1-	Virtual Reality Interface for Real-time Musical Robot Teleoperation
<br>
2-	Brain-Computer Interface
<br>
3-	AI Assistant (Technical Assistant and Creative Assistant)
<br>
4-	Embodied Sensor-Based Tool
<br>
5-	Joystick Button Control
<br><br>
According to their functionality, these tool concepts were divided into three categories:
<br><br>
a-	Control interface (1, 4, 5)
<br>
b-	Generative/data-based tool (2)
<br>
c-	AI Assistant tool (3)

  </div>
  </div>
 </div>
</div>

    
<img src="/project/img/tools.png" alt="speculative tools" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<hr class="solid">

<h2 class="special-font-h2"> <strong>PROTOTYPE</strong></h2>

<h3 class="special-fontblue-h3"> <strong>The Concepts</strong></h3>
<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
 The concepts can be summarised as:
<br><br>
<b>1-	Virtual Reality Interface for Real-time Musical Robot Teleoperation</b><br>
A controller interface to teleoperate a musical robot capable of performing certain movements, which operates a tool or plays an instrument remotely
<br><br>
<b>2-	Brain-Computer Interface</b><br>
A tool that captures brainwave data and sends it to a generative computer software which creates audio and/or visuals based on the captured data
<br><br>
<b>3-	AI Assistant (Technical Assistant and Creative Assistant)</b><br>
An intelligent tool which serves as a technical assistant to diagnose and fix problems in real-time, and as a creative assistant that suggests audio and visual patterns based on the performer’s output
<br><br>
<b>4-	Embodied Sensor-Based Tool</b><br>
An embodied controller tool which allows the artist to control the mapped parameters of a software to add elements and dynamics to the performance
<br><br>
<b>5-	Joystick-Button Control</b><br>
A physical controller joystick button which allows the artist to control the parameters of a software with a joystick and button

  </div>
  </div>
 </div>
</div>

<h3 class="special-fontblue-h3"> <strong>Concept Prototyping</strong></h3>

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
In this project, multiple prototyping techniques were used based on each tool concept. Concept prototypes/visualisations were created for each tool, and a high-fidelity clickable prototype was designed for Tool 3, which had more functionality to be evaluated.<br><br>
The first step was to create visualisations for all five concepts by reproducing them in 3D modelling software, followed by 3D computer rendering to visualise the experiential aspects of the tools. The second step was to create a high-fidelity prototype for Tool 3: AI Assistant. The third step was to animate a number of the prototypes and place all still and animated 3D prototypes in a web-based virtual reality environment to create a simulation for evaluation with the participants.
  </div>
  </div>
 </div>
</div>

<br>

<h5 class="special-fontgrey-h5">Virtual Reality Interface for Real-time Musical Robot Teleoperation</h5>

<img src="/project/img/msc-concept1.png" alt="msc concept 1" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
On the right, the artist is using a virtual reality interface during live performance, their hand and finger movements are translated as commands by the robot. On the left, the musical robot is playing the instrument in a remote location based-on the commands it receives from the user.<br><br>
Virtual Reality Interface allows the artist to teleoperate the musical robot, which performs tasks on a tool or plays an instrument in real-time. The user views the sensor-based robot’s reference frame via the head-mounted display and interacts with the tool/instrument remotely through the robot, which receives the data of the human’s movements as commands and performs the task accordingly in real-time. The output of the robot is integrated into the artist’s performance through a software. In this concept, it is assumed that the musical robot is capable of performing the movements it receives as commands from the remote user.<br><br>
An example use case can be that the artist is playing a piano that is located in their home remotely by controlling the robot with a virtual reality interface, therefore the artist is able to perform with tools/instruments they might not be able to transport to the event.
  </div>
  </div>
 </div>
 </div>

<h5 class="special-fontgrey-h5">Brain-Computer Interface</h5>
<img src="/project/img/msc-concept2.png" alt="msc concept 2" style="width:100%" class="center" onContextMenu="return false;" draggable="false">
    
<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
This concept emerged based on the responses of P2, who suggested a brainwave reader tool; P5, who imagined a tool he could communicate with telepathically; and P6, who imagined a ‘mind reader’ tool by saying:
<br><br>
<em>“In a way, I would want it to just know what I want, when I want it. Like a mind-reading machine.”</em>
<br><br>
This concept represents a tool that captures brainwave data of the artist wearing a headpiece and sends it to a generative computer software which creates audio and/or visuals based on the captured data. The artist has full control over the output and is able to manipulate it through the software before the audience hears and sees it. 
  </div>
  </div>
 </div>
 </div>

<h5 class="special-fontgrey-h5">AI Assistant</h5>
<img src="/project/img/msc-concept3-1.png" alt="msc concept 3" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
All participants expressed a need for an assistant too that can be both technical and creative.
<br><br>
<em>“A central control system/brain that can monitor the running of all the tools” (P1)</em>
<br><br>
AI Assistant was designed with two main functions. It allows the artist to have an intelligent 'technical assistant' to monitor their performance, predict, warn against and fix technical issues, as well as a 'creative assistant' which suggests audio/visual patterns based on the artist’s performance to add elements to and enhance the output.

<br><br>
<em>“A tool that makes me more inspired. A tool that can suggest variations based on my performance; this could be done through the headphones, ie. a dedicated channel for the suggestions that one can listen to during the performance. Or a visual display that can show these variations.” (P4)</em>
<br><br>
A user flow and an interactive high-fidelity wireframe were created to demonstrate the functionality of this tool and to test with the participants.
  </div>
  </div>
 </div>
 </div>

<h5 class="special-fontgrey-h5">Embodied Sensor Patches</h5>
<img src="/project/img/msc-concept4.png" alt="msc concept 4" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
<em>“A tool that can be controlled with gestures” (P1)</em>
<br><br>
P1 imagined a tool that he could control fully with gestures. Some of the similar responses came also outside the speculative thinking exercise when the participants were discussing issues around interaction. Most participants stated that they wanted to be able to use their 'body as an interface'.
<br><br>
This embodied sensor-based controller tool is a group of patches that can be placed on skin, which allow the artist to control the mapped parameters of their software by using gestures to add elements or dynamics to the performance, e.g. controlling effects parameters of a software synthesizer using finger movements.
<br><br>
<em>“I think that one of the things I would really like is more body interaction, so, movement. I was thinking, it could be like transposing the movement of the body into editing of sound” (P3)</em>
  </div>
  </div>
 </div>
 </div>

<h5 class="special-fontgrey-h5">Joystick and Button Control</h5>
<img src="/project/img/msc-concept5.png" alt="msc concept 5" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
The idea of a single button controller came from multiple participants, with slightly different ideas around functionality.<br><br>
<em>“A button to anonymise the variations based on my performance.” (P4)</em><br><br>
<em>“A button to assign to anything, then you need to press it at the right time. (P6)</em><br><br>
<em>“A big red button the audience can press and it creates a change you can respond to and play along.” (P3)</em>
<br><br>
This is a physical controller joystick-button which allows the artist to control the parameters of a software. The joystick allows the user to navigate between the parameters, while the button activates/deactivates the parameter. E.g., Controlling the effects parameters of a synthesizer by moving the joystick and applying pressure to the button.
  </div>
  </div>
 </div>
 </div>

    
 <br><br>   
<hr class="solid">

<br>

<h2 class="special-font-h2"> <strong>TEST: Evaluation</strong></h2>

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
The evaluation session was divided into two parts; a detailed presentation of the concept prototypes and discussion, and a virtual reality simulation walkthrough with conversational prompts. Two of the interview participants took part in this phase.
  </div>
  </div>
 </div>
 </div>

<h3 class="special-fontblue-h3"> <strong>Part 1: Concept Prototype Presentation and Discussion</strong></h3>

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
In the first part of the session, the five 3D-rendered concept prototypes were presented. The participants were familiar with some of the concepts since they had participated in the interviews and some of the ideas had come directly from the 'speculative thinking exercises'.
<br>
The participants were asked the following questions for each concept:
<br><br>
<b>1.</b> What are your thoughts about this tool concept? Do you like it/would you use it?
<br>
<b>2.</b> What are your favorite aspects of this concept?
<br>
<b>3.</b> Would you want to make changes, add or remove any of the features?
<br>
<b>4.</b> Would you want to change anything in the interaction?
<br><br>
Besides the concept evaluation, the participants also evaluated a high-fidelity clickable wireframe hosted on the Figma. The evaluation focused on the features of the tool rather than usability. Through screensharing, the features were presented to the participants interactively, which allowed them to observe and comment on the flow of the application as well. They were asked if they had any suggestions on the presented features, interaction and flow of the application, which revealed several changes to be made.
  </div>
  </div>
 </div>
 </div>

<h3 class="special-fontblue-h3"> <strong>Part 2: Web-Based Virtual Reality Simulation</strong></h3>

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
The goal of this part of the evaluation was to understand whether seeing the designs in 3D format in a more immersive setting would make any difference for the participants and whether the experience would generate further feedback, which could lead to valuable data. The secondary goal was to experiment with a novel research method and identify the advantages and disadvantages of web-based virtual reality simulation as a method in the evaluation stage.
<br>
In this part of the meeting, the participants were introduced to the Virtual Reality environment through screensharing and given instructions which included:
<br><br>
<b>1-</b>	Open the link: https://hubs.mozilla.com/4LZq3mF/prototype-room
<br>
<b>2-</b>	Click ‘Join Room’
<br>
<b>3-</b>	Choose/customise your avatar and name it
<br>
<b>4-</b>	Mute your microphone
<br>
<b>5-</b>	Enter the Room
<br>
Use the arrow keys for navigation
<br>
Use Q and E keys to turn left and right
  </div>
  </div>
 </div>
 </div>

<img src="/project/img/msc-vrroom.gif" alt="vr prototype room" style="width:80%" class="center" onContextMenu="return false;" draggable="false">

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
The participants joined the space with 3D avatars, where they could navigate freely and view the still and animated prototypes in 3D format.
<br><br>
In this stage, a conversational walkthrough approach was used to allow the participants some freedom of exploration in the space after going through the prototypes in detail in the previous part of the evaluation. It was based on the following four questions:
<br><br>
<b>1-</b>	You have just seen and talked about these prototypes in the Zoom session. What kind of difference does it make to see the prototypes in 3D in VR?
<br>
<b>2-</b>	Is there is any change that needs to be made, or anything added/removed to make the experience better, or to understand the prototypes better?”
<br>
<b>3-</b>	What do you like about seeing the prototypes in VR?
<br>
<b>4-</b>	Do you have any more suggestions for the prototypes themselves after seeing them here?
  </div>
  </div>
 </div>
 </div>
    
<h3 class="special-fontblue-h3"> <strong>Evaluation Results</strong></h3>

<img src="/project/img/msc-evaluation-analysis.png" alt="evaluation analysis" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
Thematic analysis of the evaluation session revealed three main themes:
<br><br>
<b>1-</b>	AI Features
<br>
<b>2-</b>	Interaction
<br>
<b>3-</b>	VR Simulation for Evaluation
<br><br>
Following the evaluation results, final user flows and designs were updated to reflect the required changes.
<br><br>
This section presents the evaluation results and the final designs in two parts. The first part focuses on the prototypes based on the data obtained in the Zoom session, and the second part details the evaluation carried out in web-based virtual reality.
  </div>
  </div>
 </div>
 </div>

<h4 class="special-font-h4">Evaluation and Final Designs Part 1</h4>
<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
<b>Concept 1: Virtual Reality Interface for Real-time Musical Robot Teleoperation</b>
<br><br>
The participants did not suggest any changes for this concept; therefore, no changes were made on the prototype.<br>
Participant 2 stated that he was not able to incorporate acoustic instruments into electronic music performances due to feedback issues on stage and this tool would solve the problem. He referred to the VR interface as a link between the artist and the remote tool:<br><br>
<em>“I guess that's in a way the link between. Like a MIDI input and robot output. That's interesting." (P2)</em><br><br>
Participant 1, who interacts with a large number of tools on stage and has more complex accessibility needs due to being a wheelchair user, stated that this tool would solve an important problem for him.<br><br>
<em>"You solved that the problem with the first tool, because then you can have a robot doing the things in your studio remotely." (P1)</em>

<br><br><br>
<b>Concept 2: Brain-Computer Interface</b>
<br><br>
The participants did not suggest any changes for this concept; therefore, no changes were made on the prototype.<br><br>
It was observed that the participants were excited about the possibility of using brainwaves as a novel interaction model.
<br><br>
<em>"This is like an evolution of the tool where you can plug these sensors on plants, vegetables and fruit. Here you apply it on your brain, that’s a step forward!" (P1)</em>

<br><br><br>
<b>Concept 3: AI Assistant</b>
<br><br>
The participants suggested that it needed additional features in the Creative Assistant functionality. In the presented prototype, it could analyse media only during the performance.
<br><br>
Participant 1 said that <em>"It should allow uploading and analysing of pre-recorded media, as well as analysing and saving the analysis of the live media during the performance, both of which could be used in a future performance."</em>
<br><br>
P2 also agreed saying:
<br>
<em>"Maybe something you can like upload to train the software and it draws similarities in those sets, and then creates an average response."</em>
<br><br>
P1 suggested that this feature would help him save time during the performance:
<em>“I would love to see something like that. When you do a live set, you're never 100%. I just go there and have to think super quick. So, I can become more focused in the listening. In a few seconds, I can do more things."</em>
<br><br>
The participants also suggested that <em>"this application should run on a separate device with touch-screen capability rather than the computer they use in the performance to immediately access the tool without interrupting the performance."</em>
<br><br>
Therefore, the user flow was updated, and the changes were made on the final designs. This change required updating five screens and designing four new screens.

  </div>
  </div>
 </div>
 </div>

<img src="/project/img/msc-concept3-2.png" alt="msc concept 5" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
Final Design
  </div>
  </div>
 </div>
 </div>
    
<center><video width=100% height=auto controls>
  <source src="/project/videos/msc-ai-video.mp4" type="video/mp4">
  <source src="movie.ogg" type="video/ogg">
</video></center>

<br><br>


<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
<b>Concept 4: Embodied Sensor Patches</b>
<br><br>
The participants did not suggest any changes on the form of the tool, however, they said that the sensor patches could have more sensory capabilities, such as <em>"sensing pressure, not only movement, which could allow more patches to be used on the body".</em> They suggested that <em>they could also press on the patches, which would trigger the parameters and the movement could be used for tasks that did not require as much immediacy.</em> They said that <em>movement sensor would not provide the accuracy when they needed to turn something on/off quickly, and if they could press on the patches, the activation would be instant, and they could use movement to add dynamics only.</em>
<br><br>
<em>"You can click once or click twice. Or press it, like three fast presses etc, and those could trigger different things, really interesting also to take it to your body." (P2)</em>

<br><br><br>
<b>Concept 5: Joystick and Button Control</b>
<br><br>
Originally, the single-button controller idea was from one of the participants who was also present at the evaluation session. However, after seeing the concept, both participants expressed that, <em>"while they liked the idea of using a single button, it would likely cause limitations and probably make the performance more error-prone due to having to navigate through the parameters with a single joystick."</em>
<br><br>
<em>"Maybe easier to have a whole handset controller, not just a joystick.." (P2)</em>
<br><br>
They suggested that a handheld controller device with more buttons could be a better option, however, they both agreed to drop the concept. Therefore, this concept was not included in the final designs. 
  </div>
  </div>
 </div>
 </div>

<h4 class="special-font-h4">Evaluation and Final Designs Part 2</h4>

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
This section details the results of the second part of the evaluation that took place in the web-based VR environment.
<br><br>
In this part, the participants joined the VR environment where they viewed the 3D still and animated prototypes and were asked to evaluate both the experience with the prototypes and the experience of viewing them in virtual reality.<br><br>
The participants stated that viewing the prototypes in virtual reality <em>"felt almost like seeing them in the real world".</em> When asked if the prototypes were clear for them, they said that <em>"they could understand what they were, while the previous part of the session had helped the experience."</em> They stated that <em>"it was important to be introduced to the prototypes prior to the virtual reality experience, especially in the case of ‘AI Assistant’; while the rest of the prototypes were easier to understand".</em> They added that <em>"it was very helpful to be able to navigate around the 3D models and see them in animated form.</em>
<br><br>
<em>"It helps of course. I see that this is the robot playing the remote instrument. “Hello robot!” (P1)</em>
  </div>
  </div>
 </div>
 </div>

<img src="/project/img/msc-vrroom2.png" alt="vr prototype room" style="width:100%" class="center" onContextMenu="return false;" draggable="false">

<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
<em>“I love that you can access it like that. I love to go into the object. It is interactive.” (P2)</em>
<br><br>
When they were asked whether they would want to see or experience anything else in this space such as text or images, to understand the prototypes better, P2 responded:
<br>
<em>“If you already know what each prototype does, like we did through the slides, you can just get straight into it, and keep it simple. Maybe if there was more information on the screen about the tools, it would be overwhelming to navigate through.” (P2)</em>
<br><br>
VR experience also created a virtual-ethnographic research opportunity. The participants were present in the space with their avatars, which made it possible to observe their movements and areas of focus. It was observed that they got close to the prototypes and were also interested in exploring the space itself. An interesting observation was that one of the participants changed their default name assigned by the platform to their artist name and customised their avatar after spending some time in the space. The other participant quickly followed him and changed his name too. This may indicate that the virtual environment felt like a 'real' environment, which motivated them to represent themselves like how they do in the physical reality as artists. They were also much more comfortable with each other compared to how they presented themselves during the Zoom discussion.
<br><br>
Both participants stated that they were curious about immersive VR experience too.
<br><br>
<em>"I would love to try with a headset too because it is already so interesting on the laptop." (P2)</em>
<br><br>
Overall, the experience was positive. The results show that web-based VR simulation created an engaging, stimulating and an effective environment for evaluating certain aspects of the prototypes. This method can be useful in evaluating the high-level aspects, the form and perceived experience of the prototypes rather than usability or other technical aspects. In this study, the emphasis was on developing new concepts/technologies, and the aim of the simulation was to enhance the evaluation stage by going beyond concept visuals and making the presentation of the concepts more immersive.
  </div>
  </div>
 </div>
 </div>

<br><br>
<hr class="solid">

    
<h2 class="special-font-h2"> <strong>FUTURE WORK</strong></h2>
<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
The research has revealed insights on the type of tools the artists envision in terms of functionality and features, as well as how they want to interact with them. The results show that there are several areas where the artists seek improvement on functionality and interaction as presented in the previous sections.<br><br>
Future work could focus on transforming the co-designed concepts into physical tools. Each concept may be the subject of a separate research project to be explored in detail, designed and developed as a fully functioning tool. This project provides another opportunity for future work by using web-based virtual reality as a research method. This can be taken further to explore elements that made the VR experience interesting and engaging for the participants in this study. These elements could include the architecture of the environment, animation to simulate experiences, and social interaction in VR. Current literature focuses mainly on immersive VR, which has presented very different results compared to this study; therefore, future work could also compare virtual reality technologies with different degrees of immersion to explore opportunities of VR as a research method in HCI.
  </div>
  </div>
 </div>
 </div>

<br><br>
<hr class="solid">

<h2 class="special-font-h2"> <strong>CONCLUSION</strong></h2>
<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
This study focused on exploring opportunities for intelligent tools and tools with alternative interaction models to enhance creativity and expressivity in digital live media performance. This report described the process of co-designing four future live media performance tools in collaboration with a group of artists and using web-based virtual reality simulation as an evaluation method.<br><br>
The literature review has shown that most of the existing research on designing live performance tools centre around very specific tools and the documentation of their design and development processes, whereas there is very little research focused on co-designing tools through exploration with the primary users. This project managed to create an environment for exploration and imagination with the users themselves to generate four innovative concepts and evaluate them with a novel method.
<br><br>
This is a step towards designing desirable, enjoyable and usable intelligent tools and interaction models that may enhance the creativity and expressivity of live performance artists.
  </div>
  </div>
 </div>
 </div>
  
<br>
<br>

<hr class="solid">

<h2 class="special-font-h2"> <strong>REFERENCES</strong></h2>
  
<div class="centerthetext">
<div class="flex-container1-text">
 <div class="row">
  <div class="column">
<p class="blogtext">
Abercrombie, N. and Longhurst, B., 1998. Audiences. A Sociological Theory of Performance and Imagination. London/Thousand Oaks/New Delhi: Sage Publications.
<br>
<br>
Amershi, S., Weld, D., Vorvoreanu, M., Fourney, A., Nushi, B., Collisson, P. Suh, J., Iqbal, S., Bennett, P., Inkpen, K., Teevan, J., Kikin-Gil, R., Horvitz, E., 2019. Guidelines for Human-AI Interaction. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, Paper 3, pp.1–13. doi:https://doi.org/10.1145/3290605.3300233
<br>
<br>
Arisona, S., 2007. Live performance tools. ACM SIGGRAPH 2007 courses on - SIGGRAPH '07, pp.73-126. doi: https://doi.org/10.1145/1281500.1281677
<br>
<br>
Bardzell, J. and Bardzell, S., 2013. “What is critical about critical design?”. Proceedings of the SIGCHI conference on human factors in computing systems, pp. 3297–3306.
<br>
<br>
Bartneck, C., Belpaeime, T., Eyssel, F., Kanda, T., Keijsers, M., Sabanovic, S., 2020. Human-Robot Interaction. pp. 126-160 https://doi.org/10.1017/9781108676649
<br>
<br>
Bassyouni, Z., Elhajj, I., 2021. Augmented Reality Meets Artificial Intelligence in Robotics: A Systematic Review. Frontiers in Robotics and AI, 8. Doi: https://doi.org/10.3389/frobt.2021.724798
<br>
<br>
Boden, M., 1998. Creativity and artificial intelligence. Artificial Intelligence, 103(1-2), pp.347-356.
<br>
<br>
Boden, M., Edmonds, E., 2009. What is generative art?. Digital Creativity, 20(1-2), pp.21-46. doi: 10.1080/14626260902867915
<br>
<br>
Bell, A., Hein, E., Ratcliffe, J., 2015. ‘Beyond Skeuomorphism: The Evolution of Music Production Software User Interface Metaphors’. Journal on the Art of Record Production. https://www.arpjournal.com/asarpwp/beyond-skeuomorphism-the-evolution-of-music-production-software-user-interface-metaphors-2/
<br>
<br>
Bijsterveld, K. and Peters, P., 2010. Composing Claims on Musical Instrument Development: A Science and Technology Studies' Contribution. Interdisciplinary Science Reviews, 35(2), pp.106-121. https://doi.org/10.1179/030801810X12723585301039
<br>
<br>
Birtchnell, T., 2018. Listening without ears: Artificial intelligence in audio mastering. Big Data & Society, 5.
<br>
<br>
Braun, V., Clarke, V., 2012. Thematic Analysis. In H. Cooper, P. M. Camic, D. L. Long, A. T. Panter, D. Rindskopf, & K. J. Sher (Eds.), APA Handbook of Research Methods in Psychology, Vol. 2. Research designs: Quantitative, Qualitative, Neuropsychological, and Biological. American Psychological Association, pp. 57–71. https://doi.org/10.1037/13620-004
<br>
<br>
Caramiaux, B.,  Donnarumma, M., 2021. Artificial Intelligence in Music and Performance: A Subjective Art-Research Inquiry. ArXiv, abs/2007.15843.
<br>
<br>
Chahl, J., Jain, L., Mizutani, A., Sato-Ilic, M., 2007. Innovations in Intelligent Machines - 1. https://doi.org/10.1007/978-3-540-72696-8
<br>
<br>
Chakraborty, S., Dutta, S., Timoney, J., 2021. The Cyborg Philharmonic: Synchronizing interactive musical performances between humans and machines. Humanities and Social Sciences Communications. 8. 74. https://doi.org/10.1057/s41599-021-00751-8
<br>
<br>
Chen, J., Hu, P., Huicheng, Z., Yang, J., Xie, J., Yakun, J., Gao, Z. & Zhang, C., 2019. Toward Intelligent Machine Tool. Engineering. 5. https://doi.org/10.1016/j.eng.2019.07.018
<br>
<br>
Cipresso, P., Giglioli, I.A.C., Raya, M.A., Riva G., 2018. The Past, Present, and Future of Virtual and Augmented Reality Research: A Network and Cluster Analysis of the Literature. Front Psychol. 6;9:2086. doi: https://doi.org/10.3389/fpsyg.2018.02086
<br>
<br>
Coeckelbergh, M., 2017. Can Machines Create Art?. Philosophy & Technology, [Online]. 30, 285–303. doi: https://doi.org/10.1007/s13347-016-0231-5
<br>
<br>
Cook, P., 2001. Principles for designing computer music controllers. In Proceedings of the 2001 Conference on New interfaces for musical expression (NIME '01). National University of Singapore, SGP, pp.1–4. doi: https://doi.org/10.5281/zenodo.1176358
<br>
<br>
Cooke, G., 2010. Start making sense: Live audio-visual media performance, International Journal of Performance Arts and Digital Media, 6:2, 193-208, doi: https://doi.org/10.1386/padm.6.2.193_1
<br>
<br>
Cooke, G., 2011. “Liveness and the machine: improvisation in live audio-visual performance.” Screen Sound, Vol.2. pp.9-26.
<br>
<br>
Correia, N. and Tanaka, A., 2021. From GUI to AVUI: Situating Audiovisual User Interfaces Within Human-Computer Interaction and Related Fields. http://dx.doi.org/10.4108/eai.12-5-2021.169913
<br>
<br>
Correia, N. and Tanaka, A., 2014. User-Centered Design of a Tool for Interactive Computer-Generated Audiovisuals. Proc. of 2nd Int. Conf. on Live Interfaces.
<br>
<br>
Deary, I., Spinath, F. & Bates, T., 2006. Genetics of intelligence. European Journal of Human Genetics. 14, pp. 690–700. https://doi.org/10.1038/sj.ejhg.5201588
<br>
<br>
Dhariwal, P., Jun, H., Payne, C., Kim, J.W., Radford, A., & Sutskever, I., 2020. Jukebox: A Generative Model for Music. ArXiv, abs/2005.00341.
<br>
<br>
Donnarumma, M., 2017. Beyond the Cyborg: Performance, attunement and autonomous computation. International Journal of Performance Arts and Digital Media. 13. https://doi.org/10.1080/14794713.2017.1338828
<br>
<br>
Dunne, A. and Raby, F., 2013. Speculative Everything: Design, Fiction, and Social Dreaming. MIT Press, Cambridge (Massachusetts)
<br>
<br>
Eaton, J., 2014. The Space Between Us : A Live Performance with Musical Score Generated via Affective Correlates Measured in EEG of One Performer and an Audience Member.
<br>
<br>
Emerson, G. and Egermann, H., 2020. ‘Exploring the motivations for building new digital musical instruments’, Musicae Scientiae, 24(3), pp. 313–329. doi: 10.1177/1029864918802983.
<br>
<br>
Evans, J., Mathur, A., 2005. The Value of Online Surveys. Internet Research. 15. pp.195-219. doi: https://doi.org/10.1108/10662240510590360
<br>
<br>
Fernández, J.D., Vico, F., 2013. AI methods in algorithmic composition: a comprehensive survey. J. Artif. Int. Res. 48, 1 (October 2013), pp. 513–582.
<br>
<br>
Gemeinboeck P., Saunders R., 2016. The Performance of Creative Machines. In: Koh J., Dunstan B., Silvera-Tawil D., Velonaki M. (eds) Cultural Robotics. CR 2015. Lecture Notes in Computer Science, vol 9549. Springer, Cham. https://doi.org/10.1007/978-3-319-42945-8_13
<br>
<br>
Gibson, I., Brown, D., Cobb, S.V., Eastgate, R.M., 2008. VIRTUAL REALITY AND RAPID PROTOTYPING: CONFLICTING OR COMPLIMENTARY?
<br>
<br>
González Aguirre, D.I., 2019. State-of-the-Art. In: Visual Perception for Humanoid Robots. Cognitive Systems Monographs, vol 38. Springer, Cham. https://doi.org/10.1007/978-3-319-97841-3_2.
<br>
<br>
Gurevich, M. and Cavan Fyans, A., 2011. Digital Musical Interactions: Performer–system relationships and their perception by spectators. Organised Sound, 16(2), pp.166-175. doi: https://doi.org/10.1017/S1355771811000112
<br>
<br>
Harwood, T., Garry, T. and Belk, R., 2019. Design fiction diegetic prototyping: a research framework for visualizing service innovations. Journal of Services Marketing, 34(1), pp.59-73. https://doi.org/10.1108/JSM-11-2018-0339
<br>
<br>
Hein, E., 2021. ‘Ableton Live 11’. Journal of the American Musicological Society 74 (1): pp.214–225. doi: https://doi.org/10.1525/jams.2021.74.1.214
<br>
<br>
Hoffman, G. and Weinberg, G., 2010. Shimon. CHI '10 Extended Abstracts on Human Factors in Computing Systems, pp.3097-3102.
<br>
<br>
Honig, W., Milanes, C., Scaria, L., Phan, T., Bolas, M. and Ayanian, N., 2015. Mixed reality for robotics. 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),.
<br>
<br>
Ilsar, A. and Hughes, M., 2020. Harmony across Music, Visuals and Movement in a New Audio-visual Gestural Performance. Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction, pp.625–630. doi: https://doi.org/10.1145/3374920.3375283
<br>
<br>
Johannessen, L., Keitsch, M., & Pettersen, I., 2019. Speculative and Critical Design — Features, Methods, and Practices. Proceedings of the Design Society: International Conference on Engineering Design, 1(1), 1623-1632. doi:10.1017/dsi.2019.168
<br>
<br>
Kato, I., Ohteru, S., Shirai, K., Matsushima, T., Narita, S., Sugano, S., Kobayashi, T., & Fujisawa, E., 1987. The robot musician 'wabot-2' (waseda robot-2). Robotics, 3, pp.143-155. https://doi.org/10.1016/0167-8493(87)90002-7
<br>
<br>
Kersting K., 2018. Machine Learning and Artificial Intelligence: Two Fellow Travelers on the Quest for Intelligent Behavior in Machines. Frontiers in big data, 1, 6. https://doi.org/10.3389/fdata.2018.00006
<br>
<br>
Kirby, D., 2009. The Future is Now: Diegetic Prototypes and the Role of Popular Films in Generating Real-World Technological Development. Social Studies of Science, 40(1), pp.41-70. https://doi.org/10.1177/0306312709338325
<br>
<br>
Kirschner, D., Velik, R., Yahyanejad, S., Brandstötter, M. and Hofbaur, M., 2016. YuMi, Come and Play with Me! A Collaborative Robot for Piecing Together a Tangram Puzzle. Lecture Notes in Computer Science, pp.243-251. https://doi.org/10.1007/978-3-319-43955-6_29.
<br>
<br>
LaViola, J., 2013. 3D Gestural Interaction: The State of the Field. ISRN Artificial Intelligence, 2013, pp.1-18. doi: https://doi.org/10.1155/2013/514641
<br>
<br>
Leigh, S.w., Sareen, H., Kao, H.-L., Liu, X., Maes, P., 2017. Body-Borne Computers as Extensions of Self. Computers. MDPI AG, 6(1), 12. doi: 10.3390/computers6010012.
<br>
<br>
Lu, S., Shpitalni, M. and Gadh, R., 1999. Virtual and Augmented Reality Technologies for Product Realization. CIRP Annals, 48(2), pp.471-495.
<br>
<br>
Lum, H. C., Elliott, L.J., Aqlan, F., Zhao, R., 2020. ‘Virtual Reality: History, Applications, and Challenges for Human Factors Research’, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 64(1), pp. 1263–1268. doi: https://doi.org/10.1177/1071181320641300
<br>
<br>
Mainsbridge, M., & Beilharz, K.A., 2014. Body As Instrument: Performing with Gestural Interfaces. NIME.
<br>
<br>
Maravita A., Iriki A., 2004. 'Tools for the body (schema)', Trends in Cognitive Sciences, 8(2), pp. 79-86. doi: https://doi.org/10.1016/j.tics.2003.12.008
<br>
<br>
McLaughlan, R., 2019. Virtual reality as a research method: is this the future of photo-elicitation?, Visual Studies, 34:3, 252-265, doi: https://doi.org/10.1080/1472586X.2019.1680315
<br>
<br>
Mulder, I. and Stappers, P., 2009. Co-creating in practice: Results and challenges. 2009 IEEE International Technology Management Conference (ICE),.
<br>
<br>
Müller, M., Günther, T., Kammer, D., Wojdziak, J., Lorenz, S. and Groh, R., 2016. Smart Prototyping - Improving the Evaluation of Design Concepts Using Virtual Reality. Lecture Notes in Computer Science, pp.47-58. doi: https://doi.org/10.1007/978-3-319-39907-2_5
<br>
<br>
Oates, B.J., 2006. Researching Information Systems and Computing. London: Sage Publications Ltd.
<br>
<br>
Ostanin, M., Yagfarov, R. and Klimchik, A., 2019. Interactive Robots Control Using Mixed Reality. IFAC-PapersOnLine, 52(13), pp.695-700. doi: https://doi.org/10.1016/j.ifacol.2019.11.307
<br>
<br>
Petersen, K., Solis, J., Takanishi, A., 2008. Toward enabling a natural interaction between human musicians and musical performance robots: Implementation of a real-time gestural interface. RO-MAN 2008 - The 17th IEEE International Symposium on Robot and Human Interactive Communication, pp.340-345. https://doi.org/10.1109/ROMAN.2008.4600689
<br>
<br>
Salter, C., (2010). Entangled: Technology and the Transformation of Performance, The MIT Press.
<br>
<br>
Sanders, E., Dandavate, U., 1999. “Design for Experiencing: New Tools,” 1st International Conference on Design and Emotion, Delft, the Netherlands. (page 90)
<br>
<br>
Sanders E, Stappers, P., 2008. Co-creation and the new landscapes of design. CoDesign 4: 5–18
<br>
<br>
Sanders, E., William, T.C., 2002. Harnessing people’s creativity: Ideation and expression through visual communication. DOI:10.1201/9780203302743.ch10
<br>
<br>
Sanders, E.B.-N. &amp; Stappers, P.J., 2014. Probes, toolkits and prototypes: Three approaches to making in codesigning. CoDesign, 10(1), pp.5–14. https://doi.org/10.1080/15710882.2014.888183
<br>
<br>
Savery, R., Zahray, L., Weinberg, G., 2020. Shimon the Rapper: A Real-Time System for Human-Robot Interactive Rap Battles. ICCC.
<br>
<br>
Sharp, H., Preece, J., Rogers, Y., 2019. Interaction Design: Beyond Human-Computer Interaction, Fifth Edition. Indianapolis: John Wiley & Sons, Inc.
<br>
<br>
Solis, J, Petersen, K, Ninomiya, T, Takeuchi, M & Takanishi, A., 2009. Development of anthropomorphic musical performance robots: From understanding the nature of music performance to its application to entertainment robotics. in 2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009., 5354547, 2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009, pp. 2309-2314, 2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009, St. Louis, MO, United States, 09/10/11. https://doi.org/10.1109/IROS.2009.5354547
<br>
<br>
Solis J., Takanishi, A., 2010. "Toward understanding the nature of musical performance and interaction with wind instrument-playing humanoids," 19th International Symposium in Robot and Human Interactive Communication, pp. 725-730, doi: 10.1109/ROMAN.2010.5598648.
<br>
<br>
Sone, Y., 2017. Japanese robot culture: performance, imagination, and modernity. Palgrave Macmillan, New York. https://doi.org/10.1057/978-1-137-52527-7
<br>
<br>
Steen, M., 2013. Co-Design as a Process of Joint Inquiry and Imagination. Design Issues; 29 (2): pp. 16–28. doi: https://doi.org/10.1162/DESI_a_00207
<br>
<br>
Sterne, J. and Razlogova, E., 2019. ‘Machine Learning in Context, or Learning from LANDR: Artificial Intelligence and the Platformization of Music Mastering’, Social Media + Society. doi: https://doi.org/10.1177/2056305119847525.
<br>
<br>
Stowell, D. and McLean, A., 2013. Live Music-Making: A Rich Open Task Requires a Rich Open Interface. Music and Human-Computer Interaction, pp.139-152. doi: https://doi.org/10.1007/978-1-4471-2990-5_8
<br>
<br>
Troisi, A., 2019. OB-scene, a Live Audio/Visual Performance for Photoplethysmograph and Female Body. Organised Sound, 23(3), 270-276. doi: https://doi.org/10.1017/S1355771818000171
<br>
<br>
Visser, F. S., Stappers, P. J., van der Lugt, R. and Sanders, E. B.-N., 2005 ‘Contextmapping: experiences from practice’, CoDesign. Informa UK Limited, 1(2), pp. 119–149. doi: https://doi.org/10.1080/15710880500135987
<br>
<br>
Voutsinas, J., Haefeli, S., 2017. The mi. mu Gloves: Finding agency in electronic music performance through ancillary gestural semiotics. Presentation at the 2017 James J Whalen Academic Symposium, New York, NY, USA. Available at: https://digitalcommons.ithaca.edu/cgi/viewcontent.cgi?article=1181&context=whalen
<br>
<br>
Weinberg, G. and Driscoll, S., 2006. Robot-human interaction with an anthropomorphic percussionist. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp.1229-1232.
<br>
<br>
Weinberg, G., Driscoll, S., 2006. Toward Robotic Musicianship. Computer Music Journal; 30 (4): pp. 28–45. https://doi.org/10.1162/comj.2006.30.4.28
<br>
<br>
Whitney, D., Rosen, E., Ullman, D., Phillips, E. and Tellex, S., 2018. ROS Reality: A Virtual Reality Framework Using Consumer-Grade Hardware for ROS-Enabled Robots. 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp.1-9. https://doi.org/10.1109/IROS.2018.8593513
<br>
<br>
Zamenopoulos, T. and Alexiou,  K., 2018. Co-design As Collaborative Research.   Connected Communities Foundation Series. Bristol: Bristol University/AHRC Connected Communities Programme.
<br>
<br>
Zimmerman, T., Lanier, J., Blanchard, C., Bryson, S. and Harvill, Y., 1987. A hand gesture interface device. Proceedings of the SIGCHI/GI conference on Human factors in computing systems and graphics interface - CHI '87. pp.189–192. doi: https://doi.org/10.1145/29933.275628
</p>
  </div>
 </div>
</div>
</div>
    
<br>
<br>


  </main>

</body>

</html>

